
'''

What is prompt engineering in the context of Large Language Models (LLMs)?

Training the model on a large data set

Adjusting the hyperparameters of the model

Iteratively refining the ask to elicit a desired response

Adding more layers to the neural network

--> C


Which statement accurately reflects the differences between these approaches in terms of the number of parameters modified and the type of data used?

Fine-tuning and continuous pretraining both modify all parameters and use labeled, task-specific data.

Parameter Efficient Fine Tuning and Soft prompting modify all parameters of the model using unlabeled data.

Soft prompting and continuous pretraining are both methods that require no modification to the original parameters of the model.

Fine-tuning modifies all parameters using labeled, task-specific data, whereas Parameter Efficient Fine-Tuning updates a few, new parameters also with labeled, task-specific data.

--> D


What does in-context learning in Large Language Models involve?

Conditioning the model with task-specific instructions or demonstrations

Training the model using reinforcement learning

Adding more layers to the model

Pretraining the model on a specific domain

--> A


What is the role of temperature in the decoding process of a Large Language Model (LLM)?

To decide to which part of speech the next word should belong

To determine the number of words to generate in a single decoding step

To adjust the sharpness of probability distribution over vocabulary when selecting the next word

To increase the accuracy of the most likely word in the vocabulary

--> C


What does the term "hallucination" refer to in the context of Language Large Models (LLMs)?

The model's ability to generate imaginative and creative content

The process by which the model visualizes and describes images in detail

The phenomenon where the model generates factually incorrect information or unrelated content as if it were true

A technique used to enhance the model's performance on specific tasks

--> C

'''